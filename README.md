International Semiconductor Research and Development Capabilities: Technical Details Supplement
This companion document to International Semiconductor Research and Development Capabilities: An Analysis Framework and Interactive Database is intended to provide technical coverage of the specific tools used in its process to facilitate reproducible results and to enable the extension of the work carried out here. All code used, raw data files, and templates are available on GitHub (https://github.com/roomtempfusion/ISRDC). To replicate or follow the process laid out here, the programming languages Python and SQL are required. Additionally, the data visualization software Tableau was extensively used, as well as Microsoft Excel. 

The first step is to retrieve the data (or use the data.csv file) from IEEE Xplore. The relevant data can be downloaded directly from IEEE using the ‘Export’ search results feature. Since this feature is capped at 2000 results per download, multiple downloads either manually or via a scripting program are required. Once the data has been downloaded into a CSV format, the processor.py file then removes extraneous (for this project specifically) information and processes the remaining data into a more usable format. This file uses countrydict.json to convert country information into a standardized list of countries. The JSON was generated by the countrymatcher.ipynb file and manually checked for validity, but other methods exist like string matching and NLP as mentioned in the Methodology section. 

Next, the countryindexer.py file then will split the processed data (or data_processed.csv) into individual CSV files for each country passed. Two features to note: the current set-up uses the JSON file mentioned above to convert the country names into the same format as in the data file. The file currently has most countries for the various State Department regions already encoded as lists, and all countries present in the dataset. Using the individual country files, lists of institutions can be constructed by simply combining all the terms in the lists within the ‘Author Affiliations’ column, and papers below a certain threshold of citations can also be removed either by sorting by citation count in Excel or with simple data frame indexing in Python. 

As previously mentioned, this data came from the USPTO. The patentview.py file queries the USPTO API for information based on the provided query. This query can be changed directly in the file, both in terms of keywords and fields to return, but the file currently is set up to process the specific fields already encoded. The file also includes the ability to query patents from a single country and from multiple, with lists corresponding to State Department regions also present. As with the IEEE data, the resulting patent data can be used to construct a list of institutions to manually search. 


The next step, manually carrying out data collection on various institutions, involves the use of a custom-built Excel template (dataentry_template.xlsx). This template uses a standardized list of terms and enforces data validation via the use of drop-down menus, allowing for most data to be structured and substantially easier to work with. The full instructions for use are included in the file. Additionally, the ISRDCdb.db file has all of the manually collected data from this project. 

Once the manual data template is received, the next step is to process the data for storage in the SQL database. This step is technically optional since working with CSV files or some other storage format is possible, but for this project, SQL was used. No edits to the template should be required, simply using the sql_import_processor.py file is sufficient to convert the templates to CSV files that can be uploaded into SQL. An alternative solution is to convert the files directly into SQL within the script, but this functionality was not used in this project. Once the templates are converted into CSV, uploading them to a SQL database is a matter of whatever SQL software is used. This project used SQLiteStudio and DB Browser for SQLite. The database structure can be found in the scmapping.db file. 

The next step is to generate data collaborations between each pair of countries. As mentioned in the Future Improvements section of the main report, there is room for improvement here, with the development of better techniques to match institutions automatically and aggregate publications. For this project, only the country associated with each institution was used, however. Using the country_collabs.py file on the processed IEEE data will generate a CSV listing the collaborations between each country. This statistic is not the number of papers that each country collaborated on, but rather the total number of citations from those papers, creating a ‘weighted collaborations’ statistic. The ISRDCdb.db file has the data about country collaborations (the value in the database is on a log 10 scale). 

Another option is to use the collabs.py file on the processed IEEE data, which will generate a CSV with each institution listed next to its country and the total number of citations. This is similar to the above, except that the total number of citations does not correspond to country-country pairs but rather to the specific institution-institution pairs. Collaborations are generated by matching institutions that are listed together on various papers and summing the total citation count from those papers, for all institutions. However, this does not account for institutions with multiple spellings, abbreviations, etc., so further processing is needed. 

The inst_processor.py file will apply some basic string processing techniques, such as removing punctuation and stop words, and match institutions based on the processed name, combining total collaboration counts for any institutions that are then found to be the same.  However, because the string processing techniques used will not account for all possible alternative names that institutions might have, the processing phase will not always be accurate and will require manual correction/geocoding or other techniques, which is why the first option (collaborations based on country) was used for this project. See the Future Improvements section in the main report for a further discussion about this. 

Finally, after all the data is assembled, it can be loaded into Tableau to construct a visualization. Tableau will automatically generate most of the relevant features, but additional features can be added fairly easily. An optional feature that helped prevent ambiguity of locations within Tableau was to use the locator.ipynb script to hard-code coordinates for each institution, but this is by no means the only option. 

For more information or questions, please contact Humza Khan at humzakhan2026@u.northwestern.edu (contact also available on GitHub). 
